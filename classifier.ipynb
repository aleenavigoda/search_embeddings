{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTS AND SETUP\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from supabase import create_client, Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FETCH PREDEFINED EMBEDDINGS AND LABELS\n",
    "\n",
    "import numpy as np\n",
    "from supabase import create_client, Client\n",
    "\n",
    "# Supabase setup\n",
    "url = \"https://hyxoojvfuuvjcukjohyi.supabase.co\"\n",
    "key = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Imh5eG9vanZmdXV2amN1a2pvaHlpIiwicm9sZSI6ImFub24iLCJpYXQiOjE3MjgzMTU4ODMsImV4cCI6MjA0Mzg5MTg4M30.eBQ3JLM9ddCmPeVq_cMIE4qmm9hqr_HaSwR88wDK8w0\"\n",
    "supabase: Client = create_client(url, key)\n",
    "\n",
    "def fetch_predefined_embeddings_and_labels():\n",
    "    \"\"\"\n",
    "    Fetch the pre-defined genre embeddings and genre_id labels from the genre_assignments table.\n",
    "    \"\"\"\n",
    "    response = supabase.table(\"genre_assignments\").select(\"genre_embedding, genre_id\").execute()\n",
    "    \n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    \n",
    "    # Iterate over the rows of data from genre_assignments\n",
    "    for row in response.data:\n",
    "        embedding = row['genre_embedding']\n",
    "        if isinstance(embedding, list) and all(isinstance(x, (int, float)) for x in embedding):\n",
    "            embeddings.append(embedding)\n",
    "            labels.append(row['genre_id'])  # Genre ID is the label\n",
    "        else:\n",
    "            print(f\"Skipping invalid embedding for genre_id {row['genre_id']}\")\n",
    "    \n",
    "    return np.array(embeddings, dtype=float), labels\n",
    "\n",
    "# Fetch pre-defined embeddings and labels\n",
    "predefined_embeddings, predefined_labels = fetch_predefined_embeddings_and_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEFINE AND TRAIN CLASSIFIER [SIMPLE]\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define a simple feedforward classifier model\n",
    "class EmbeddingClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(EmbeddingClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)  # First fully connected layer\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, num_classes)  # Output layer with number of classes (genres)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Training function\n",
    "def train_classifier(embeddings, labels, input_size, num_classes, epochs=10, batch_size=32, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Train the classifier on the embeddings.\n",
    "\n",
    "    :param embeddings: Pre-generated embeddings.\n",
    "    :param labels: Corresponding genre labels for each embedding.\n",
    "    :param input_size: Size of the embedding vector (e.g., 768).\n",
    "    :param num_classes: Number of genres (e.g., 5 genres).\n",
    "    :param epochs: Number of training epochs.\n",
    "    :param batch_size: Size of the training batch.\n",
    "    :param learning_rate: Learning rate for the optimizer.\n",
    "    :return: Trained classifier model.\n",
    "    \"\"\"\n",
    "    # Convert embeddings and labels to PyTorch tensors\n",
    "    embeddings_tensor = torch.tensor(embeddings, dtype=torch.float32)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long)  # Long is needed for classification targets\n",
    "    \n",
    "    # Create a DataLoader for batching\n",
    "    dataset = TensorDataset(embeddings_tensor, labels_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Initialize the classifier model\n",
    "    model = EmbeddingClassifier(input_size=input_size, num_classes=num_classes)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()  # Set the model to training mode\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch_embeddings, batch_labels in dataloader:\n",
    "            # Forward pass\n",
    "            outputs = model(batch_embeddings)\n",
    "            loss = loss_fn(outputs, batch_labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss/len(dataloader)}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Fetch pre-defined embeddings and labels from Step 1 (the genre_assignments table)\n",
    "predefined_embeddings, predefined_labels = fetch_predefined_embeddings_and_labels()\n",
    "\n",
    "# Call the function to train the model on the pre-defined embeddings and labels\n",
    "input_size = predefined_embeddings.shape[1]  # The size of your embedding vector\n",
    "num_classes = 5  # Assuming 5 genres\n",
    "\n",
    "# Train the model using the pre-defined data\n",
    "model = train_classifier(predefined_embeddings, predefined_labels, input_size=input_size, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD FINE-TUNED BERT MODEL\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_path = \"./fine_tuned_bert\"  # Path to fine-tuned BERT model in the current search_embeddings folder\n",
    "\n",
    "# Load the fine-tuned tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts fetched: 996\n",
      "First text: In previous eras, simulated reality had cracks: fissures in the film, graininess, discoloration. These elements kept us from falling straight into the image. Today, not only has the simulation arrived at a higher resolution of life, it feels more real. Baudrillard’s paranoia that reality had already disintegrated in the 1980s unleashed an ensuing theoretical abyss with no triumphant successor. Yet, in retrospect, it seems he didn’t quite grasp the pace at which life, the image, and perception would merge, such that individuals must navigate a new tension between these opposing forces.\n",
      "\n",
      "We are currently wrestling with the surfacing of an automated surveillance culture, marked by identity fragmentation and privacy erosion due to online platforms and new technologies like omnipresent sensors, biometrics and ambient intelligence, a phenomenon this essay terms the ambient self. Simultaneously, there exists a nostalgia for a seemingly simpler past. However, this past was not without its complexities, as exemplified by Edward Bernays' 1947 concept of \"the engineering of consent.\" This historical notion mirrors today's scenario, where consent is often manufactured through work created by computer science engineers, reflecting a continuity in the manipulation of public opinion. This period is contrasted with the present, where, as Arthur and Marilouise Kroker notes, the process often involves \"rendering marginalized bodies as accidental roadkill on the way to a technological future,\" (1) indicating a shift towards a more precise, integrated, and individual level of social engineering.\n",
      "\n",
      "In our time, we often unknowingly dwell within the digital ambience of anonymous avatars, bots, spiders, and deepfakes that will slowly make us the minority in the new reality that is being programmed. We are slowly moving into the ambient background of a digitized, non-participatory communicative environment. It is an environment where the illusion of connection belies a deeper solitude, reflecting Sherry Turkle's notion of being \"alone together\" as we rely more on technology than each other or our shared public. This is aside from the emerging omnipresence of predictive algorithms slowly supplanting the very desires they attempt to imitate. In the future, there will come a time when we will lament our present psychological apathy, which surrenders itself to emerging aberrations, including accelerated manifestations of digital drift. Our psyche, even if once vibrant, may ultimately decline into a mere archaeological site, a relic or ruin of what previously was. Gradually, we find ourselves disassembled, our most valuable components displaced and redeployed.\n",
      "\n",
      "We are condemned to fight from within the very machine that wields this power to comprehend us better than we comprehend ourselves, rather than being able to maintain ourselves as distinct from it. The digital, like Nietzsche’s mask, is now experienced as a relief, since taking it off for too long in today’s online-centered world would alienate us from our dreams and everyone we love.(2) However, contrary to Nietzsche, God died not in the 19th century but in the 20th and 21st centuries, when the intrusive thoughts the previous culture ensured no longer resonated and gave birth to a new technology of the self. We can now announce that the self, the psyche, or as Kroker says man itself as previously congealed is dying in the 21st century, to be replaced by androids in the future. (3,4)\n",
      "\n",
      "In Paul Virilio’s 1994 book The Vision Machine, both his era and, speculatively speaking, today’s too, was characterized as one dominated by synthetic vision, the intricate digital image processing inherent in computer-aided designs, and the deployment of surveillance cameras in public spaces. In this contemporary period, the foundational concept revolves around the notion of ambient intelligence, which is predicated upon the construction of comprehensive intelligence with the ultimate aim of establishing a ubiquitous network of intelligent apparatuses, encompassing smart devices, nanotechnology, and sensors, integrated seamlessly into every facet of existence and within the very fabric of all existing objects. Life, under these conditions, morphs into an ambient state, characterized predominantly by its integration with and processing through peripheral sensors. This shift signifies a profound transformation in the nature of life itself, where living is perpetually under observation and analysis, blurring the lines between the self and sensor. (5,6)\n",
      "\n",
      "In today’s world, psychological and linguistic researchers who scramble to preserve a prior sense of reality find themselves in the center of a new universe defined by the screen. (7) Like Kroker before them, contemporary criminologists call this digital drift, where highly-online youth and other second-generation digital natives increasingly blur material and virtual reality until they become criminalized, overcome by what Virilio might call an emerging glaucoma reality. (8) The screen, in essence, interpellates them into these anemic prisons, from the perspective of the architectural mirror to that of the internet data center. Now, the future as a whole is becoming consumed by the logic of the image.\n",
      "\n",
      "In the past, we had never imagined normalizing this degree of biopolitical pleasure; the latest aberrations expressed now as the digital death drive, or as vlogger Zach Willmore meta-virilizing their HIV status. Another committed suicide after his exchanges with an AI chatbot in police reports referred to on a first name basis as “Pierre”. Payton S. Gendron, for example, livestreamed themselves performing a mass shooting, while still yet another unnamed person, who offered their life for sale online and took it for money they couldn’t spend in the afterlife. (9,10,11)\n",
      "\n",
      "While we may think this technology won’t turn on us, companies are spending millions on the bet it will happen to many now reading this. For those up-to-date with the current media landscape, it is not difficult to imagine oneself on a screen in Times Square, exposed as the newest criminal-victim, watching deep fake pornography of your friends like a popular gaming streamer. (12,13) No one will want to go viral when a new start-up mass markets blackmail, should those investing in the development of these technologies succeed. By manipulating our digital twins into potential criminals, they will be able to hold us ransom in a world where accusations outpace evidence. (14) Something like the film Minority Report, it could become a “precog” world where committing a crime is already an established fact when it isn’t, but nevertheless waiting to be revealed by the next data breach. In this absurd double reality, you are being murdered, you are murdering someone else, while many other impossible scenarios and timelines coexist as well.\n",
      "\n",
      "Our glaucomatous reality appears then, as a post-mortem jumble of “headless positions” where truths and rational theories becomes arbitrary choices of the flavor du jour. (15) In this context, language inherently possesses the qualities of partiality and clarity–evident in the variations of language across different cultures and it’s inability to capture the full complexity of reality. (16) Arguably though, the ambient technological context emerging in more fragmentary, layered, and remote registers is no different than the medieval serf who often couldn’t speak beyond an isolated village tongue. (17) It’s not digital neoliberalism, but the ongoing churn of heterogeneous cultures homogenizing into a new meta-form of localization, governed by whatever new technology suddenly arrives on the scene, rather than through relational negotiation and collective cultural emergence. It’s a kind of neomedievalism where we may have conflicts between our lord, our faith, our king, or our family, a level of contradiction in our loyalties and beliefs throughout the whole chain, the stack of unseen difference. (18)\n",
      "\n",
      "During the early 1990s, the grunge documentary \"The Year Punk Broke\" included an anecdote from a fan who coined the term \"Thurstonitis\" to describe the headspace of Thurston Moore. It referred to a condition where one loses complete control of their mind and walks around in a dazed state. Like how Cobain’s last moments caught on tape show him going out in a state of headlessness.(19) I’m talking about the moment in the film where he is introduced as Kevin Costner, then spirals out of control, throwing a champagne bottle across the room. The bottle crashes so loudly that when I watched it, I could barely process the reason for the senseless violence. I lost my mind watching it for the first time on a copied tape I procured from local record store Dead Pan Alley. The story reverberates in a more tragic tone even than those of Mark Fisher and Kurt Cobain. This counterculture rerun was ritualized and mythologized on platforms like LiveJournal, Tumblr, and MySpace, paving the way to parasocialize cultural critics into over-saturated caricatures of themselves.\n",
      "\n",
      "I also thought about the Challenger space shuttle, a peak of human intelligence and failure in which blood and burnt hydrogen fuel soaked the air. In my own traumatized headspace, I watched that tape as my brother’s eyes fell into the back of his head, much as mine did. Blood and burnt black tar heroin polluted the air with sirens in the background of both scenes. Cobain, the star of the first wave of opioid epidemic faced by the marginalized, the dropouts, those who lose custody of their children, those thrust into premature death by falling completely off the screen. From his perspective, they would witness the collapse of this fabricated reality, which, for some, represents a voluntary choice. In contrast, for others, particularly those marginalized, the offline, inherited reality persists as an unwelcome imposition. In this inherited reality, many, especially those on the fringes of society, have long experienced and continue to feel a profound sense of powerlessness and lack of control.\n",
      "\n",
      "This digital world of hyper-realism is constituted by a social structure fractured by fake virality and multifocal arousal bringing forth a new culture marked by the unprecedented addiction and loneliness. The human psyche in this environment finds solace in identities as vaporous as a gas or fog. We now stand on the precipice of a truly post-reality era unlike any Baudrillard could have imagined, a simulation we keep trying to prolong, a pervasive game of a new kind.\n",
      "\n",
      "At best this would trigger grief, but it’s unlikely we would register the moments in our own past of such glaucomatous phenomena, where Poe’s Law induces gender dysphoria and digital drift induces transracialism a no man’s land where Cobain and Costner are indistinguishably fused characters. Our persistence, however, the human instinct for survival and purpose, is also tied to our ability to die faster and harder collectively because there is no final defense. Even if we detox from drugs or overcome internet porn-addicted childhoods, we find ourselves grappling with the bifurcation between the criminal-victims of our stolen reality and the perpetrators and the casualties of its destruction and commodification.\n",
      "\n",
      "Acknowledgements: I want to thank J.M. Adams, Jacqui Lucente, Guy Mackinnon-Little, and Corrine Ciani for helping edit this essay as well as input from Jak Ritger, Keaton Slansky, Scott Litts, Brandon Avery Joyce, Gardiner Hallowell, Samuel Eagleton, Matthew Binder, Stefan Kostic, Ben Dreith, Nicolas Sanchez, Charlie Yates, and many others who collectively helped bring this to this advanced stage.\n"
     ]
    }
   ],
   "source": [
    "## FETCH FULL TEXT DATA FROM URLS TABLE\n",
    "\n",
    "def fetch_full_texts_from_supabase():\n",
    "    \"\"\"\n",
    "    Fetch the full texts from the urls_table where genre_id is NULL or empty.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Fetch rows where genre_id is NULL (empty)\n",
    "        data = supabase.table('urls_table')\\\n",
    "            .select('id, full_text')\\\n",
    "            .is_('genre_id', None).execute()  # Check for NULL in genre_id\n",
    "\n",
    "        if data.data:\n",
    "            return data.data  # Return the list of dictionaries containing 'id' and 'content'\n",
    "        else:\n",
    "            print(\"No rows fetched. Check if genre_id has empty values.\")\n",
    "            return []\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch full texts: {e}\")\n",
    "        return []\n",
    "    \n",
    "    # Debugging: Print the number of texts fetched\n",
    "print(f\"Number of texts fetched: {len(texts)}\")\n",
    "print(f\"First text: {texts[0] if len(texts) > 0 else 'No texts available'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PREDICT GENRES WITH FINE-TUNED BERT\n",
    "\n",
    "def predict_genres_with_bert(model, texts, tokenizer, confidence_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Predict genres for the given texts using the fine-tuned BERT model.\n",
    "\n",
    "    :param model: The pre-trained BERT model.\n",
    "    :param texts: The original text content to classify.\n",
    "    :param tokenizer: The tokenizer for BERT.\n",
    "    :param confidence_threshold: The confidence threshold for genre assignment.\n",
    "    :return: List of predicted genres.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    genre_assignments = []\n",
    "\n",
    "    for text in texts:\n",
    "        # Tokenize the input text\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "        # Run the model on the tokenized input\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probabilities = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        # Assign genres based on the probabilities\n",
    "        max_prob, predicted_genre = torch.max(probabilities, dim=1)\n",
    "        if max_prob >= confidence_threshold:\n",
    "            genre_assignments.append(predicted_genre.item())\n",
    "        else:\n",
    "            genre_assignments.append(5)  # Assign genre 5 as fallback for low confidence\n",
    " \n",
    "    return genre_assignments\n",
    "\n",
    "# Predict genres for the full text using the fine-tuned BERT model\n",
    "predicted_genres = predict_genres_with_bert(model, texts, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PREDICT GENRES WITH CONFIDENCE THRESHOLD DEBUGGING\n",
    "\n",
    "def predict_genres_with_bert(model, texts, tokenizer, confidence_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Predict genres for the given texts using the fine-tuned BERT model.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    genre_assignments = []\n",
    "\n",
    "    for text in texts:\n",
    "        # Tokenize the input text\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "        # Run the model on the tokenized input\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probabilities = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        # Assign genres based on the probabilities\n",
    "        max_prob, predicted_genre = torch.max(probabilities, dim=1)\n",
    "\n",
    "        # Print confidence score (max_prob) for debugging\n",
    "        print(f\"Confidence score for text: {max_prob.item()}\")\n",
    "\n",
    "        if max_prob >= confidence_threshold:\n",
    "            genre_assignments.append(predicted_genre.item())\n",
    "        else:\n",
    "            genre_assignments.append(5)  # Assign genre 5 as fallback for low confidence\n",
    "    \n",
    "    return genre_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PREDICT GENRE FUNCTION v3\n",
    "\n",
    "def predict_genres_with_bert(model, texts, tokenizer, confidence_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Predict genres for the given texts using the fine-tuned BERT model.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    genre_assignments = []\n",
    "\n",
    "    for text in texts:\n",
    "        print(f\"Processing text: {text[:50]}...\")  # Print the first 50 characters of each text\n",
    "\n",
    "        # Tokenize the input text\n",
    "        try:\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "            print(f\"Tokenized input: {inputs}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Tokenization failed for text: {text[:50]}... with error: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Run the model on the tokenized input\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                logits = outputs.logits\n",
    "                probabilities = torch.softmax(logits, dim=-1)\n",
    "                print(f\"Model output probabilities: {probabilities}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Model inference failed for text: {text[:50]}... with error: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Assign genres based on the probabilities\n",
    "        max_prob, predicted_genre = torch.max(probabilities, dim=1)\n",
    "\n",
    "        # Print confidence score (max_prob) for debugging\n",
    "        print(f\"Confidence score for text: {max_prob.item()}\")\n",
    "\n",
    "        if max_prob >= confidence_threshold:\n",
    "            genre_assignments.append(predicted_genre.item())\n",
    "        else:\n",
    "            genre_assignments.append(5)  # Assign genre 5 as fallback for low confidence\n",
    "    \n",
    "    return genre_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UPDATE GENRE IDS IN URLS TABLE\n",
    "\n",
    "def update_genre_ids_in_urls_table(essay_ids, predicted_genres):\n",
    "    \"\"\"\n",
    "    Update the genre_id in the urls_table based on the predicted genres.\n",
    "\n",
    "    :param essay_ids: List of essay IDs from the urls_table (remaining_ids).\n",
    "    :param predicted_genres: List of predicted genres corresponding to each essay ID.\n",
    "    \"\"\"\n",
    "    for id, genre_id in zip(essay_ids, predicted_genres):\n",
    "        # Update the genre_id for the matching essay ID\n",
    "        response = supabase.table(\"urls_table\").update({\"genre_id\": genre_id}).eq(\"id\", id).execute()\n",
    "        \n",
    "        # Check for errors in the update process\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to update id {id}: {response.json()}\")\n",
    "        else:\n",
    "            print(f\"Successfully updated id {id} with genre {genre_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_genre_ids_in_urls_table(essay_ids, predicted_genres):\n",
    "    \"\"\"\n",
    "    Update the genre_id in the urls_table based on the predicted genres.\n",
    "\n",
    "    :param essay_ids: List of essay IDs from the urls_table (remaining_ids).\n",
    "    :param predicted_genres: List of predicted genres corresponding to each essay ID.\n",
    "    \"\"\"\n",
    "    for id, genre_id in zip(essay_ids, predicted_genres):\n",
    "        # Ensure id is an integer and genre_id is numeric (float)\n",
    "        id = int(id)  # Cast id to int (int4 in Supabase)\n",
    "        genre_id = float(genre_id)  # Cast genre_id to float (numeric in Supabase)\n",
    "\n",
    "        # Debugging: Print what is about to be updated\n",
    "        print(f\"Attempting to update id {id} with genre {genre_id}\")\n",
    "        \n",
    "        # Update the genre_id for the matching essay ID\n",
    "        response = supabase.table(\"urls_table\").update({\"genre_id\": genre_id}).eq(\"id\", id).execute()\n",
    "\n",
    "        # Print the response to inspect it\n",
    "        print(f\"Response for id {id}: {response}\")\n",
    "\n",
    "# Update urls_table with the predicted genres\n",
    "update_genre_ids_in_urls_table(remaining_ids, predicted_genres)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
